{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGxVJY00wcqZ",
        "outputId": "c2d91e4f-1f3d-4ca1-efa6-0c405f0fad26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pymorphy2\n",
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    Doc\n",
        ")\n",
        "import spacy\n",
        "import torch  # использую flair\n",
        "!pip install flair\n",
        "import flair\n",
        "import nltk; nltk.download('stopwords')\n",
        "!python3 -m spacy download en\n",
        "import re\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/19/902d1691c1963ab8c9a9578abc2d65c63aa1ecf4f8200143b5ef91ace6f5/flair-0.6.1-py3-none-any.whl (331kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 2.7MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.2.2)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
            "Collecting transformers>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 15.5MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.5MB/s \n",
            "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ea/01/47358efec5396fc80f98273c42cbdfe7aab056252b07884ffcc0f118978f/konoha-4.6.2-py3-none-any.whl\n",
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0+cu101)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from flair) (4.2.6)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/91/77/3f0f53856e86af32b1d3c86652815277f7b5f880002584eb30db115b6df5/bpemb-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.7)\n",
            "Collecting pytest>=5.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/36/9e022b76a3ac440e1d750c64fa6152469f988efe0c568b945e396e2693b5/pytest-6.1.1-py3-none-any.whl (272kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 37.4MB/s \n",
            "\u001b[?25hCollecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.41.1)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 48.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.8.1)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 49.0MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (2.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Collecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 36.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (20.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (3.12.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Collecting overrides==3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/42/8d/caa729f809ecdf8e76fac3c1ff7d3f0b72c398c9dd8a6919927a30a873b3/overrides-3.0.0.tar.gz\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (0.16.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (20.2.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.1.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.10.1)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (2.0.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.9.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.0.0->flair) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.0->flair) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers>=3.0.0->flair) (50.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair) (3.2.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Building wheels for collected packages: mpld3, ftfy, segtok, langdetect, sqlitedict, sacremoses, overrides\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116677 sha256=34167186701ba3b745b901389f76dd0dcf0a6ca22cd35873126c9f7afd355d7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45612 sha256=11c74d618f833e718b1ccfbd43444579519198e3a9b73603a50a7af2f9aca17d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25021 sha256=03a0ef485b5c84c5ff59aff3a080f2601ef459fdc990ac7c5def83d1fcd43075\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993195 sha256=63de49287c244aecb0b2a3df770b3d9431dfc188e5700c283521dcb6f5010a12\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp36-none-any.whl size=14377 sha256=7aeedcf370f5f154e20781e2553f3cfab0dda4baf9abb74ba49c56b5c56b8278\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=04ff4e0a6547824bda1c9adab5276103d73aeac32e5dd88598408cdab0686e57\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.0.0-cp36-none-any.whl size=5669 sha256=320e34535cfe0201e8d2561008f138af5a7535ebbdc41e526097fd6cf46d3ed7\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/1b/ec/6c71a1eb823df7f850d956b2d8c50a6d49c191e1063d73b9be\n",
            "Successfully built mpld3 ftfy segtok langdetect sqlitedict sacremoses overrides\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: mpld3, deprecated, tokenizers, sentencepiece, sacremoses, transformers, ftfy, overrides, konoha, janome, bpemb, pluggy, pytest, segtok, langdetect, sqlitedict, flair\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed bpemb-0.3.2 deprecated-1.2.10 flair-0.6.1 ftfy-5.8 janome-0.4.1 konoha-4.6.2 langdetect-1.0.8 mpld3-0.3 overrides-3.0.0 pluggy-0.13.1 pytest-6.1.1 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.91 sqlitedict-1.7.0 tokenizers-0.9.2 transformers-3.4.0\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhZetqzwxCLE"
      },
      "source": [
        "# сначала поработаю с русским\n",
        "text_ru = 'К счастью, больной был прооперирован удачно. Новый факультет, образованный в институте по приказу министерства, пользуется большой популярностью. Девушка очень образованна. Больной котенок нашел дом и выздоровел! Морозна ночь, все небо ясно. Он не заметил нас и прошел мимо, хотя вокруг было относительно светло, и мне стало всё ясно. Мой друг снова прошел мимо меня, не поздоровавшись, зато новые друзья были мне рады. Что же ты стоишь? Что ты будешь делать? Он сказал мне, что не приедет. Он ушел, как стемнело. Она обратила внимание на то, как стало темно. У меня в руках была ручная пила. Три дня я пила только воду.'\n",
        "text_ru = text_ru.lower()  # так будет удобнее сравнивать рез-ты, потому что pymorphy сам приводит всё к нижнему регистру\n",
        "\n",
        "# чищу и токенизирую для pymorphy и mystem\n",
        "ru_text_clean = re.sub('[₽#&-;\\*\\(\\(,.!?\\d]', ' ', text_ru)\n",
        "ru_text_split = ru_text_clean.split()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5bWsY6BxMRD"
      },
      "source": [
        "ru_pymorphy_raw = []  # использую pymorphy\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "for i in range(len(ru_text_split)):\n",
        "    ru_pymorphy_raw.append(morph.parse(ru_text_split[i]))\n",
        "ru_pymorphy_str = str(ru_pymorphy_raw)\n",
        "# привожу к единому стандарту. не стала делать функцию, тк у всех теггеров разные теги…\n",
        "ru_pymorphy_str = re.sub('ADJ[SF]', 'ADJ', ru_pymorphy_str)\n",
        "ru_pymorphy_str = re.sub('PRT[SF]', 'VERB', ru_pymorphy_str)\n",
        "ru_pymorphy_str = re.sub('GRND', 'VERB', ru_pymorphy_str)\n",
        "ru_pymorphy_str = re.sub('INFN', 'VERB', ru_pymorphy_str)\n",
        "ru_pymorphy_str = re.sub('ADVB', 'ADV', ru_pymorphy_str)\n",
        "ru_pymorphy_str = re.sub('PRCL', 'PTLC', ru_pymorphy_str)\n",
        "ru_pymorphy_str = re.sub('NPRO', 'PRO', ru_pymorphy_str)\n",
        "ru_pymorphy_str = re.sub('NUMR', 'NUM', ru_pymorphy_str)\n",
        "ru_pymorphy = re.findall('\\[Parse\\(word=\\'([а-яё]+)\\',\\stag=OpencorporaTag\\(\\'([A-Z]+)', ru_pymorphy_str)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOwM5FNmy9_s",
        "outputId": "7f563986-321e-42cc-a711-0ecabba85a43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        }
      },
      "source": [
        "from pymystem3 import Mystem\n",
        "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linu..\n",
        "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!cp mystem /root/.local/bin/mystem\n",
        "\n",
        "m = Mystem()  # использую mystem. в колабе что-то тормозит (было написано что-то про установку mystem откуда-то…) \n",
        "ru_text_str = str(ru_text_split)  # нашла решение, которое приписала выше, но оно тоже не помогает. \n",
        "ru_mystem_raw = m.analyze(ru_text_str)  # в след. ячейке пропишу от руки то, что получила в этом месте кода \n",
        "ru_mystem_str = str(ru_mystem_raw)  # + добавлю в реп файл «2.py», где есть вся русская часть кода. эх, а хотела всё красиво собрать в одном месте…\n",
        "ru_mystem_str = re.sub('S', 'NOUN', ru_mystem_str)  # привожу к единому стандарту\n",
        "ru_mystem_str = re.sub('A', 'ADJ', ru_mystem_str)\n",
        "ru_mystem_str = re.sub('PR', 'PREP', ru_mystem_str)\n",
        "ru_mystem_str = re.sub('V', 'VERB', ru_mystem_str)\n",
        "ru_mystem_str = re.sub('[AS]PRO', 'PRO', ru_mystem_str)\n",
        "ru_mystem_str = re.sub('PART', 'PTCL', ru_mystem_str)\n",
        "ru_mystem = re.findall('text\\':\\s\\'([а-яА-Яё]+)\\',\\s\\'[a-z]+\\':\\s\\[\\{\\'wt\\':\\s[\\d\\.]+,\\s\\'lex\\':\\s\\'[а-яё]+\\',\\s\\'gr\\':\\s\\'([A-Z]+)', ru_mystem_str)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-21 01:35:25--  http://download.cdn.yandex.net/mystem/mystem-3.0-linu..\n",
            "Resolving download.cdn.yandex.net (download.cdn.yandex.net)... 5.45.205.242, 5.45.205.245, 5.45.205.244, ...\n",
            "Connecting to download.cdn.yandex.net (download.cdn.yandex.net)|5.45.205.242|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://cache-mskmar02.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linu.. [following]\n",
            "--2020-10-21 01:35:26--  http://cache-mskmar02.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linu..\n",
            "Resolving cache-mskmar02.cdn.yandex.net (cache-mskmar02.cdn.yandex.net)... 5.45.222.26, 2a02:6b8:0:2b03::e2\n",
            "Connecting to cache-mskmar02.cdn.yandex.net (cache-mskmar02.cdn.yandex.net)|5.45.222.26|:80... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2020-10-21 01:35:27 ERROR 404: Not Found.\n",
            "\n",
            "tar: mystem-3.0-linux3.1-64bit.tar.gz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "cp: cannot stat 'mystem': No such file or directory\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-71c6941edb03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# использую mystem. в колабе что-то тормозит (было написано что-то про установку mystem откуда-то…)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mru_text_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mru_text_split\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# но я это писала в pycharm, и там всё работало. если что вот мой тг: t.me/nstsi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mru_mystem_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mru_text_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mru_mystem_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mru_mystem_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mru_mystem_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'S'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NOUN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mru_mystem_str\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# привожу к единому стандарту\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_analyze_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36m_analyze_impl\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    291\u001b[0m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                     \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-uv77gRzoS6"
      },
      "source": [
        "ru_mystem = [('к', 'PREP'), ('счастью', 'NOUN'), ('больной', 'ADJ'), ('был', 'VERB'), ('прооперирован', 'VERB'), ('удачно', 'ADJDVERB'), ('новый', 'ADJ'), ('факультет', 'NOUN'), ('образованный', 'ADJ'), ('в', 'PREP'), ('институте', 'NOUN'), ('по', 'PREP'), ('приказу', 'NOUN'), ('министерства', 'NOUN'), ('пользуется', 'VERB'), ('большой', 'ADJ'), ('популярностью', 'NOUN'), ('девушка', 'NOUN'), ('очень', 'ADJDVERB'), ('образованна', 'ADJ'), ('больной', 'ADJ'), ('котенок', 'NOUN'), ('нашел', 'VERB'), ('дом', 'NOUN'), ('и', 'CONJ'), ('выздоровел', 'VERB'), ('морозна', 'ADJ'), ('ночь', 'NOUN'), ('все', 'NOUNPREPO'), ('небо', 'NOUN'), ('ясно', 'ADJDVERB'), ('он', 'NOUNPREPO'), ('не', 'PADJRT'), ('заметил', 'VERB'), ('нас', 'NOUNPREPO'), ('и', 'CONJ'), ('прошел', 'VERB'), ('мимо', 'ADJDVERB'), ('хотя', 'CONJ'), ('вокруг', 'ADJDVERB'), ('было', 'VERB'), ('относительно', 'ADJDVERB'), ('светло', 'ADJDVERB'), ('и', 'CONJ'), ('мне', 'NOUNPREPO'), ('стало', 'VERB'), ('всё', 'ADJDVERB'), ('ясно', 'ADJDVERB'), ('мой', 'ADJPREPO'), ('друг', 'NOUN'), ('снова', 'ADJDVERB'), ('прошел', 'VERB'), ('мимо', 'PREP'), ('меня', 'NOUNPREPO'), ('не', 'PADJRT'), ('поздоровавшись', 'VERB'), ('зато', 'CONJ'), ('новые', 'ADJ'), ('друзья', 'NOUN'), ('были', 'VERB'), ('мне', 'NOUNPREPO'), ('рады', 'ADJ'), ('что', 'NOUNPREPO'), ('же', 'PADJRT'), ('ты', 'NOUNPREPO'), ('стоишь', 'VERB'), ('что', 'CONJ'), ('ты', 'NOUNPREPO'), ('будешь', 'VERB'), ('делать', 'VERB'), ('он', 'NOUNPREPO'), ('сказал', 'VERB'), ('мне', 'NOUNPREPO'), ('что', 'CONJ'), ('не', 'PADJRT'), ('приедет', 'VERB'), ('он', 'NOUNPREPO'), ('ушел', 'VERB'), ('как', 'CONJ'), ('стемнело', 'VERB'), ('она', 'NOUNPREPO'), ('обратила', 'VERB'), ('внимание', 'NOUN'), ('на', 'PREP'), ('то', 'NOUNPREPO'), ('как', 'CONJ'), ('стало', 'VERB'), ('темно', 'ADJDVERB'), ('у', 'PREP'), ('меня', 'NOUNPREPO'), ('в', 'PREP'), ('руках', 'NOUN'), ('была', 'VERB'), ('ручная', 'ADJ'), ('пила', 'NOUN'), ('три', 'NUM'), ('дня', 'NOUN'), ('я', 'NOUNPREPO'), ('пила', 'VERB'), ('только', 'PADJRT'), ('воду', 'NOUN')]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrmo3sTixRMA"
      },
      "source": [
        "segmenter = Segmenter()  # использую natasha\n",
        "morph_vocab = MorphVocab()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "doc = Doc(text_ru)\n",
        "doc.segment(segmenter)\n",
        "doc.tag_morph(morph_tagger)\n",
        "ru_natasha_raw = doc.tokens\n",
        "ru_natasha_str = str(ru_natasha_raw)\n",
        "ru_natasha_str = re.sub('ADP', 'PREP', ru_natasha_str)  # привожу к единому стандарту\n",
        "ru_natasha_str = re.sub('AUX', 'VERB', ru_natasha_str)\n",
        "ru_natasha_str = re.sub('CCONJ', 'CONJ', ru_natasha_str)\n",
        "ru_natasha_str = re.sub('SCONJ', 'CONJ', ru_natasha_str)\n",
        "ru_natasha_str = re.sub('PRON', 'PRO', ru_natasha_str)\n",
        "ru_natasha_str = re.sub('PART', 'PTCL', ru_natasha_str)\n",
        "ru_natasha = re.findall('text=\\'([а-яА-Яё]+)\\',\\spos=\\'([A-Z]+)', ru_natasha_str)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYCMBTszxdDl"
      },
      "source": [
        "#теперь ручная разметка. разберемся сразу с обоими текстами:\n",
        "ru_manually_raw = 'к PREP счастью NOUN больной NOUN был VERB прооперирован VERB удачно ADV новый ADJ факультет NOUN образованный VERB в PREP институте NOUN по PREP приказу NOUN министерства NOUN пользуется VERB большой ADJ популярностью NOUN девушка NOUN очень ADV образованная ADJ больной ADJ котенок NOUN нашел VERB дом NOUN и CONJ выздоровел VERB морозна ADJ ночь NOUN все ADJ небо NOUN ясно ADJ он PRO не PTCL заметил VERB нас PRO и CONJ прошел VERB мимо ADV хотя CONJ вокруг PREP было VERB относительно ADV светло ADV и CONJ мне PRO стало VERB всё NOUN ясно ADV мой PRO друг NOUN снова ADV прошел VERB мимо PREP меня PRO не PTCL поздоровавшись VERB зато CONJ новые ADJ друзья NOUN были VERB мне PRO рады ADJ что ADV же PTCL ты PRO стоишь VERB что PRO ты PRO будешь VERB делать VERB он PRO сказал VERB мне PRO что CONJ не PTCL приедет VERB он PRO ушел VERB как CONJ стемнело VERB она PRO обратила VERB внимание NOUN на PREP то PRO как PRO стало VERB темно ADV у PREP меня PRO в PREP руках NOUN была VERB ручная ADJ пила NOUN три NUM дня NOUN я PRO пила VERB только ADV воду NOUN'\n",
        "en_manually_raw = 'Back VERB up ADV and CONJ give VERB me PRO an ART answer NOUN Stop VERB and CONJ answer VERB my PRO question NOUN He PRO wants VERB to PREP frame VERB me PRO for PREP stealing VERB the ART gold ADJ frame NOUN If CONJ you PRO set VERB us PRO on PREP fire NOUN the ART boss NOUN will VERB definitely ADV fire VERB you PRO I PRO saw VERB a ART giant ADJ saw NOUN outside ADV the ART shelter NOUN it PRO destroyed VERB the ART tree NOUN that CONJ was VERB about PREP to PREP flower VERB Why ADV did VERB you PRO fool VERB me PRO into PREP believing VERB that CONJ this ART function NOUN would VERB work VERB Could VERB you PRO please ADV iron VERB my PRO suit NOUN Did VERB I PRO tell VERB you PRO that CONJ they PRO paid VERB me PRO a ART visit NOUN the ART other ADJ night NOUN when ADV I PRO was VERB trying VERB to PREP finally ADV get VERB some ADJ sleep NOUN Email VERB me PRO later ADV and CONJ we PRO will VERB discuss VERB that ART email NOUN Can VERB you PRO guess VERB what ADJ type NOUN of PREP glue NOUN I PRO used VERB to PREP do VERB this ART object NOUN Wake VERB up ADV and CONJ help VERB me PRO to PREP find VERB my PRO watch NOUN'\n",
        "\n",
        "\n",
        "def manually_into_list(string_raw):  # преобразовываю ручную разметку в годный для сравненеия вид\n",
        "    string_raw = string_raw.split()\n",
        "    list = []\n",
        "    for k in range(len(string_raw)):\n",
        "        if (k != 0 and k % 2 == 1):\n",
        "            pass\n",
        "        else:\n",
        "            list.append((string_raw[k], string_raw[k + 1]))\n",
        "    return list\n",
        "\n",
        "\n",
        "ru_manually = manually_into_list(ru_manually_raw)\n",
        "en_manually = manually_into_list(en_manually_raw)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMWnzKNjxoUK",
        "outputId": "a2ac6fec-153e-48ce-f34d-4bbc85ae3f94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def intersection(lst1, lst2):\n",
        "    lst3 = [value for value in lst1 if value in lst2]\n",
        "    return len(lst3)\n",
        "\n",
        "\n",
        "def accuracy_check(list1, list2):  # функция вычисления точности\n",
        "    accuracy = round((intersection(list1, list2)/101), 2)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "pymorphy_accuracy = accuracy_check(ru_pymorphy, ru_manually)  # считаю\n",
        "mystem_accuracy = accuracy_check(ru_mystem, ru_manually)\n",
        "natasha_accuracy = accuracy_check(ru_natasha, ru_manually)\n",
        "\n",
        "if pymorphy_accuracy > mystem_accuracy:  # находит и объявляет самый точный русский теггер\n",
        "    if pymorphy_accuracy > natasha_accuracy:\n",
        "        print('pymorphy самый точный из русских:', pymorphy_accuracy, ' vs ', mystem_accuracy, ' и ', natasha_accuracy)\n",
        "    else:\n",
        "        print('natasha самая точная из русских:', natasha_accuracy, ' vs ', pymorphy_accuracy, ' и ', mystem_accuracy)\n",
        "elif pymorphy_accuracy < mystem_accuracy:\n",
        "    if mystem_accuracy > natasha_accuracy:\n",
        "        print('mystem самый точный из русских:', mystem_accuracy, ' vs ', pymorphy_accuracy, ' и ', natasha_accuracy)\n",
        "    else:\n",
        "        print('natasha самая точная из русских:', natasha_accuracy, ' vs ', pymorphy_accuracy, ' и ', mystem_accuracy)\n",
        "else:\n",
        "    if pymorphy_accuracy > natasha_accuracy:\n",
        "        print('pymorphy и mystem самые точные из русских:', pymorphy_accuracy, ' и ', mystem_accuracy, ' vs ', natasha_accuracy)\n",
        "    else:\n",
        "        print('natasha самая точная из русских:', natasha_accuracy, ' vs ', pymorphy_accuracy, ' и ', mystem_accuracy)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "natasha самая точная из русских: 0.91  vs  0.84  и  0.65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW4vqdYn0Ggc"
      },
      "source": [
        "# перехожу к английскому\n",
        "text_en = 'Back up and give me an answer. Stop and answer my question. He wants to frame me for stealing the gold frame. If you set us on fire the boss will definitely fire you. I saw a giant saw outside the shelter, it destroyed the tree that was about to flower. Why did you fool me into believing that this function would work? Could you please iron my suit? Did I tell you that they paid me a visit the other night when I was trying to finally get some sleep? Email me later and we will discuss that email. Can you guess what type of glue I used to do this object? Wake up and help me to find my watch!'\n",
        "\n",
        "\n",
        "def standart(text_str):  # привожу к единому стандарту. в этот раз функция, потому что все 3 теггера используют одинаковые тэги (отличные от моих)\n",
        "    text_str = re.sub('RB', 'ADV', text_str)\n",
        "    text_str = re.sub('CC', 'CONJ', text_str)\n",
        "    text_str = re.sub('VB[ZGPD]', 'ADV', text_str)\n",
        "    text_str = re.sub('VB', 'VERB', text_str)\n",
        "    text_str = re.sub('MD', 'VERB', text_str)\n",
        "    text_str = re.sub('PRP', 'PRO', text_str)\n",
        "    text_str = re.sub('PRP\\$', 'PRO', text_str)\n",
        "    text_str = re.sub('DT', 'ART', text_str)\n",
        "    text_str = re.sub('NN', 'NOUN', text_str)\n",
        "    text_str = re.sub('TO', 'PREP', text_str)\n",
        "    text_str = re.sub('IN', 'PREP', text_str)\n",
        "    text_str = re.sub('WDT', 'CONJ', text_str)\n",
        "    text_str = re.sub('JJ', 'ADJ', text_str)\n",
        "    text_str = re.sub('WRB', 'ADV', text_str)\n",
        "    text_str = re.sub('NNP', 'PRO', text_str)\n",
        "    return text_str"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp3tpZpk0Nvx"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")  # начинаю со spacy\n",
        "\n",
        "en_text_clean = re.sub('[₽#&-;\\*\\(\\(,.!?\\d]', '', text_en)\n",
        "doc = nlp(en_text_clean)\n",
        "en_spacy_list = []\n",
        "en_spacy_str = ''\n",
        "for token in doc:  # проще было бы сделать сразу список списков, но нужно будет менять теги, чтоб совпадали с моими, поэтому нужна str (для re.sub)\n",
        "    en_spacy_str += token.text\n",
        "    en_spacy_str += ' '\n",
        "    en_spacy_str += token.tag_\n",
        "    en_spacy_str += ' '\n",
        "\n",
        "en_spacy_str = standart(en_spacy_str)\n",
        "en_spacy = re.findall('([a-zA-Z]+)\\s([A-Z]+)', en_spacy_str)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTgmyvej0rKV",
        "outputId": "ea0954c9-dc0d-4132-f9bc-a3278ef1581a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "from flair.data import Sentence  # использую flair\n",
        "from flair.models import SequenceTagger\n",
        "nltk.download('punkt')\n",
        "\n",
        "pos = SequenceTagger.load('pos-fast')\n",
        "\n",
        "en_flair_raw = []\n",
        "en_flair_list = nltk.tokenize.sent_tokenize(text_en)\n",
        "\n",
        "for i in tqdm(en_flair_list):\n",
        "  sentence = Sentence(i)\n",
        "  pos.predict(sentence)\n",
        "  en_flair_raw.append(sentence.to_tagged_string())\n",
        "\n",
        "import re\n",
        "en_flair_str = str(en_flair_raw)\n",
        "en_flair_str = standart(en_flair_str)\n",
        "en_flair = re.findall('([a-zA-Z]+)\\s<([A-Z]+)', en_flair_str)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "2020-10-21 01:58:12,837 loading file /root/.flair/models/en-pos-ontonotes-fast-v0.5.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:01<00:00,  8.57it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S66E8tlm07QX",
        "outputId": "509ac69d-4ddc-435f-db8d-09cc7cd6d336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "nltk.download('tagsets')  # использую nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import word_tokenize\n",
        "text = word_tokenize(text_en)\n",
        "en_nltk_raw = nltk.pos_tag(text)\n",
        "en_nltk_str = str(en_nltk_raw)\n",
        "en_nltk_str = standart(en_nltk_str)\n",
        "en_nltk = re.findall('\\'([a-zA-Z]+)\\',\\s\\'([A-Z]+)', en_nltk_str)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[('Back', 'NOUNP'), ('up', 'ADV'), ('and', 'CONJ'), ('give', 'VERB'), ('me', 'PRO'), ('an', 'ART'), ('answer', 'NOUN'), ('Stop', 'NOUNP'), ('and', 'CONJ'), ('answer', 'VERB'), ('my', 'PRO'), ('question', 'NOUN'), ('He', 'PRO'), ('wants', 'ADV'), ('to', 'PREP'), ('frame', 'VERB'), ('me', 'PRO'), ('for', 'PREP'), ('stealing', 'ADV'), ('the', 'ART'), ('gold', 'NOUN'), ('frame', 'NOUN'), ('If', 'PREP'), ('you', 'PRO'), ('set', 'ADV'), ('us', 'PRO'), ('on', 'PREP'), ('fire', 'NOUN'), ('the', 'ART'), ('boss', 'NOUN'), ('will', 'VERB'), ('definitely', 'ADV'), ('fire', 'VERB'), ('you', 'PRO'), ('I', 'PRO'), ('saw', 'ADV'), ('a', 'ART'), ('giant', 'ADJ'), ('saw', 'NOUN'), ('outside', 'PREP'), ('the', 'ART'), ('shelter', 'NOUN'), ('it', 'PRO'), ('destroyed', 'ADV'), ('the', 'ART'), ('tree', 'NOUN'), ('that', 'WART'), ('was', 'ADV'), ('about', 'PREP'), ('to', 'PREP'), ('flower', 'VERB'), ('Why', 'WADV'), ('did', 'ADV'), ('you', 'PRO'), ('fool', 'VERB'), ('me', 'PRO'), ('into', 'PREP'), ('believing', 'ADV'), ('that', 'PREP'), ('this', 'ART'), ('function', 'NOUN'), ('would', 'VERB'), ('work', 'VERB'), ('Could', 'NOUNP'), ('you', 'PRO'), ('please', 'ADV'), ('iron', 'VERB'), ('my', 'PRO'), ('suit', 'NOUN'), ('Did', 'NOUNP'), ('I', 'PRO'), ('tell', 'ADV'), ('you', 'PRO'), ('that', 'PREP'), ('they', 'PRO'), ('paid', 'ADV'), ('me', 'PRO'), ('a', 'ART'), ('visit', 'NOUN'), ('the', 'ART'), ('other', 'ADJ'), ('night', 'NOUN'), ('when', 'WADV'), ('I', 'PRO'), ('was', 'ADV'), ('trying', 'ADV'), ('to', 'PREP'), ('finally', 'ADV'), ('get', 'VERB'), ('some', 'ART'), ('sleep', 'NOUN'), ('Email', 'NOUNP'), ('me', 'PRO'), ('later', 'ADV'), ('and', 'CONJ'), ('we', 'PRO'), ('will', 'VERB'), ('discuss', 'VERB'), ('that', 'ART'), ('email', 'NOUN'), ('Can', 'VERB'), ('you', 'PRO'), ('guess', 'VERB'), ('what', 'WP'), ('type', 'NOUN'), ('of', 'PREP'), ('glue', 'NOUN'), ('I', 'PRO'), ('used', 'ADV'), ('to', 'PREP'), ('do', 'VERB'), ('this', 'ART'), ('object', 'VERB'), ('Wake', 'VERB'), ('up', 'RP'), ('and', 'CONJ'), ('help', 'VERB'), ('me', 'PRO'), ('to', 'PREP'), ('find', 'VERB'), ('my', 'PRO'), ('watch', 'NOUN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKFJQhTQ20mt",
        "outputId": "ef0a6666-e470-4d64-9a4a-5caa2361d61b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "spacy_accuracy = accuracy_check(en_spacy, en_manually)  # считаю\n",
        "flair_accuracy = accuracy_check(en_flair, en_manually)\n",
        "nltk_accuracy = accuracy_check(en_nltk, en_manually)\n",
        "\n",
        "if spacy_accuracy > flair_accuracy:  # находит и объявляет самый точный английский теггер\n",
        "    if spacy_accuracy > nltk_accuracy:\n",
        "        print('spacy самый точный из английских:', spacy_accuracy, ' vs ', flair_accuracy, ' и ', nltk_accuracy)\n",
        "    else:\n",
        "        print('nltk самая точная из английских:', nltk_accuracy, ' vs ', spacy_accuracy, ' и ', flair_accuracy)\n",
        "elif spacy_accuracy < flair_accuracy:\n",
        "    if flair_accuracy > nltk_accuracy:\n",
        "        print('flair самый точный из английских:', flair_accuracy, ' vs ', spacy_accuracy, ' и ', nltk_accuracy)\n",
        "    else:\n",
        "        print('nltk самая точная из английских:', nltk_accuracy, ' vs ', spacy_accuracy, ' и ', flair_accuracy)\n",
        "else:\n",
        "    if spacy_accuracy > nltk_accuracy:\n",
        "        print('spacy и flair самые точные из английских:', spacy_accuracy, ' и ', flair_accuracy, ' vs ', nltk_accuracy)\n",
        "    else:\n",
        "        print('nltk самая точная из английских:', nltk_accuracy, ' vs ', spacy_accuracy, ' и ', flair_accuracy)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "flair самый точный из английских: 0.92  vs  0.9  и  0.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gDa05Th2wt-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}